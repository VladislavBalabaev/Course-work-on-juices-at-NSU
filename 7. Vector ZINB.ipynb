{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DOWNLOADING DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import pprint\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import offline\n",
    "import statsmodels.graphics.tsaplots as tsa\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tomato_discount = pd.read_csv('Dataframes/tomato_discount.csv')\n",
    "tomato_7 = pd.read_csv('Dataframes/tomato_7.csv')\n",
    "\n",
    "# tomato_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.760910\n",
      "         Iterations: 41\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 44\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.122793\n",
      "         Iterations: 38\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 41\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.151392\n",
      "         Iterations: 39\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 42\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.024207\n",
      "         Iterations: 32\n",
      "         Function evaluations: 35\n",
      "         Gradient evaluations: 35\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.227683\n",
      "         Iterations: 54\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 69\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.186321\n",
      "         Iterations: 52\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.366966\n",
      "         Iterations: 38\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 41\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.958843\n",
      "         Iterations: 37\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 40\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.354663\n",
      "         Iterations: 47\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.140529\n",
      "         Iterations: 37\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 40\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.533201\n",
      "         Iterations: 44\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.761740\n",
      "         Iterations: 51\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.077879\n",
      "         Iterations: 65\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 77\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.583996\n",
      "         Iterations: 87\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 93\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.894115\n",
      "         Iterations: 26\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.197246\n",
      "         Iterations: 35\n",
      "         Function evaluations: 38\n",
      "         Gradient evaluations: 38\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.089024\n",
      "         Iterations: 38\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 41\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211614\n",
      "         Iterations: 47\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.918215\n",
      "         Iterations: 62\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 78\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.216036\n",
      "         Iterations: 40\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 43\n",
      "TOTAL MSE: 124.18941999481305\n"
     ]
    }
   ],
   "source": [
    "# CALCULATING CLEAR DEMAND\n",
    "\n",
    "IDs = [col.strip('is_discount_') for col in tomato_discount.columns if 'is_discount_' in col]\n",
    "ZINB_non_price_models =[]\n",
    "MSE = []\n",
    "\n",
    "for id in IDs:\n",
    "    endog = tomato_7['salesvolume_' + id]\n",
    "    exog = tomato_7[['av_price_' + id, 'is_supplied_' + id]]\n",
    "    ZINB_non_price_models.append(\n",
    "        sm.ZeroInflatedNegativeBinomialP(endog = endog, exog = exog, exog_infl = exog, inflation = 'logit').fit(maxiter = 100))\n",
    "    non_price_demand = ZINB_non_price_models[IDs.index(id)].predict(exog, exog_infl = exog)\n",
    "\n",
    "    tomato_7['non_price_demand_' + id] = tomato_7['salesvolume_' + id] - non_price_demand\n",
    "\n",
    "    MSE.append(metrics.mean_squared_error(tomato_7['salesvolume_' + id], non_price_demand))\n",
    "\n",
    "print(f'TOTAL MSE: {sum(MSE)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# SETTING Xs, ys, IDs\n",
    "\n",
    "lags_on_Xs = []\n",
    "Xs = []; ys = []\n",
    "\n",
    "def sum_of_last(series, n = 7):\n",
    "    ser_list = []\n",
    "\n",
    "    for i in range(0, n):\n",
    "        ser_list.append(0)\n",
    "\n",
    "    for i in range(n, len(series)):\n",
    "        ser_list.append(np.sum([j for j in series[(i - n):i]]))\n",
    "\n",
    "    return ser_list\n",
    "\n",
    "for id in IDs:\n",
    "    tomato_7_process = tomato_7.copy()\n",
    "\n",
    "    # adding num_other_discounts_\n",
    "    tomato_7_process = tomato_7_process.merge(tomato_discount[['date', 'num_other_discounts_' + id]], on = 'date')\n",
    "\n",
    "    # rename target columns\n",
    "    tomato_7_process.rename(columns = {'salesvolume_' + id: 'target',\n",
    "                                        'non_price_demand_' + id: 'tar_non_price',\n",
    "                                        'av_price_' + id:'tar_price',\n",
    "                                        'num_other_discounts_' + id: 'tar_other_discounts',\n",
    "                                        'is_supplied_' + id: 'tar_is_supp'}, inplace = True)\n",
    "\n",
    "    # drop salesvolume_\n",
    "    to_drop = [col for col in tomato_7_process.columns if 'salesvolume_' in col]   # 'non_price_demand_'\n",
    "    tomato_7_process.drop(to_drop, axis = 1, inplace = True)\n",
    "\n",
    "    # shifting back of non_price_demand_\n",
    "    work_with = [col for col in tomato_7_process.columns if 'non_price_demand_' in col]    # 'salesvolume_'\n",
    "    tomato_7_process.loc[:, work_with] = tomato_7_process.loc[:, work_with].shift(-1)\n",
    "    tomato_7_process = tomato_7_process.iloc[:-1, :].reset_index(drop = True)\n",
    "\n",
    "    for col in work_with:\n",
    "        tomato_7_process.rename(columns = {col: col + '_lag'}, inplace = True)\n",
    "\n",
    "    # prices only when is supplied\n",
    "    to_supp = [col for col in tomato_7_process.columns if 'av_price_' in col]\n",
    "    for col in to_supp:\n",
    "        tomato_7_process[col] = tomato_7_process[col] * tomato_7_process['is_supplied_' + col.replace('av_price_', '')]\n",
    "\n",
    "    # adding lags on target variables\n",
    "    lags_on_X = [[*tsa.pacf(tomato_7_process['target'])].index(sorted([*tsa.pacf(tomato_7_process['target'])])[::-1][lag]) for lag in [1, 2, 3]]\n",
    "    lags_on_Xs.append(lags_on_X.copy())\n",
    "\n",
    "    for i in lags_on_X:\n",
    "        for col in ['tar_non_price', 'tar_price']:\n",
    "            tomato_7_process[col + '_' + str(i)] = tomato_7_process[col].shift(i)\n",
    "\n",
    "    tomato_7_process = tomato_7_process.iloc[10:, :].reset_index(drop = True)\n",
    "\n",
    "    # adding week statistics\n",
    "    tomato_7_process['tar_non_price_week'] = sum_of_last(tomato_7_process['tar_non_price'], n = 7)\n",
    "    tomato_7_process['non_price_demand_week'] = \\\n",
    "        sum_of_last(tomato_7_process[[col for col in tomato_7_process.columns if 'non_price_demand_' in col]].sum(axis = 1), n = 7)\n",
    "    tomato_7_process = tomato_7_process.iloc[7:, :].reset_index(drop = True)\n",
    "\n",
    "    # changing the order\n",
    "    tomato_7_process = tomato_7_process\\\n",
    "                            .loc[:, [c for c in tomato_7_process.columns if 'tar' in c] +\n",
    "                                    [c for c in tomato_7_process.columns if 'tar' not in c]]\n",
    "\n",
    "    # setting X and y\n",
    "    Xs.append(tomato_7_process.drop(['target', 'tar_non_price', 'date'], axis = 1).copy())\n",
    "    ys.append(tomato_7_process['target'].copy())\n",
    "\n",
    "    # setting date\n",
    "    if id == IDs[0]:\n",
    "        date = tomato_7_process['date']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def timesplit(X, y, train_size = 0.85):\n",
    "    n_train = int(len(y) * train_size)\n",
    "    return X[:n_train], X[n_train:], y[:n_train], y[n_train:]\n",
    "\n",
    "X_trains = []; X_tests = []; y_trains = []; y_tests = []\n",
    "\n",
    "for i in range(0, len(Xs)):\n",
    "    X_train, X_test, y_train, y_test = timesplit(Xs[i], ys[i])\n",
    "    X_trains.append(X_train.copy()); X_tests.append(X_test.copy()); y_trains.append(y_train.copy()); y_tests.append(y_test.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:30<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.557453\n",
      "         Iterations: 100\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 104\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.171817\n",
      "         Iterations: 100\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 105\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 7\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 4\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 116\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 11.499844\n",
      "         Iterations: 22\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 3\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.233199\n",
      "         Iterations: 100\n",
      "         Function evaluations: 106\n",
      "         Gradient evaluations: 106\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.962326\n",
      "         Iterations: 100\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 104\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.179245\n",
      "         Iterations: 100\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1009: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1333: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1334: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n",
      "C:\\Users\\Владислав\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1243: RuntimeWarning:\n",
      "\n",
      "overflow encountered in exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN result encountered.\n",
      "         Current function value: nan\n",
      "         Iterations: 0\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 2\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.530756\n",
      "         Iterations: 100\n",
      "         Function evaluations: 105\n",
      "         Gradient evaluations: 105\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 3\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.166698\n",
      "         Iterations: 100\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 104\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 8\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 3\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.007034\n",
      "         Iterations: 100\n",
      "         Function evaluations: 116\n",
      "         Gradient evaluations: 116\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.891512\n",
      "         Iterations: 91\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 94\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.260766\n",
      "         Iterations: 85\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 90\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.689587\n",
      "         Iterations: 100\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 120\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 1.145170\n",
      "         Iterations: 100\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 108\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\4918~1\\AppData\\Local\\Temp/ipykernel_9860/177614792.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     73\u001B[0m                                          exog_infl = X_tests[i][NB2_signif_vars[i]])  for i  in range(0, len(Xs))]\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m \u001B[0mMSE_tests\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean_squared_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_tests\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mZINB_test_preds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'MSE_tests_sum: {sum(MSE_tests)}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\4918~1\\AppData\\Local\\Temp/ipykernel_9860/177614792.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     73\u001B[0m                                          exog_infl = X_tests[i][NB2_signif_vars[i]])  for i  in range(0, len(Xs))]\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m \u001B[0mMSE_tests\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean_squared_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_tests\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mZINB_test_preds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'MSE_tests_sum: {sum(MSE_tests)}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001B[0m in \u001B[0;36mmean_squared_error\u001B[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001B[0m\n\u001B[0;32m    436\u001B[0m     \u001B[1;36m0.825\u001B[0m\u001B[1;33m...\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m     \"\"\"\n\u001B[1;32m--> 438\u001B[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001B[0m\u001B[0;32m    439\u001B[0m         \u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmultioutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    440\u001B[0m     )\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001B[0m in \u001B[0;36m_check_reg_targets\u001B[1;34m(y_true, y_pred, multioutput, dtype)\u001B[0m\n\u001B[0;32m     94\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[0my_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mensure_2d\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m     \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mensure_2d\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mforce_all_finite\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 800\u001B[1;33m             \u001B[0m_assert_all_finite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_nan\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mforce_all_finite\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"allow-nan\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    801\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    802\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mensure_min_samples\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype)\u001B[0m\n\u001B[0;32m    112\u001B[0m         ):\n\u001B[0;32m    113\u001B[0m             \u001B[0mtype_err\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"infinity\"\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mallow_nan\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"NaN, infinity\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 114\u001B[1;33m             raise ValueError(\n\u001B[0m\u001B[0;32m    115\u001B[0m                 msg_err.format(\n\u001B[0;32m    116\u001B[0m                     \u001B[0mtype_err\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmsg_dtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mmsg_dtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# FITTING ZERO INFLATED NEGATIVE BINOMIAL MODELS\n",
    "\n",
    "# https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4\n",
    "\n",
    "def NB2_fitting_i(X_trains, y_trains, i):\n",
    "    X_in_for = pd.DataFrame()\n",
    "\n",
    "    def block(y, X):\n",
    "        Poisson_reg = sm.GLM(y, X, family = sm.families.Poisson())\\\n",
    "            .fit()\n",
    "\n",
    "        df_train = pd.concat([y, X], join = 'outer', axis = 1)\n",
    "        df_train['BB_LAMBDA'] = Poisson_reg.mu\n",
    "        df_train['AUX_OLS_DEP'] = df_train.apply(lambda x: ((x['target'] - x['BB_LAMBDA']) ** 2 - x['BB_LAMBDA']) / x['BB_LAMBDA'], axis = 1)\n",
    "\n",
    "        aux_olsr_results = smf.ols(\"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\", df_train)\\\n",
    "            .fit()\n",
    "        alpha = aux_olsr_results.params[0] if aux_olsr_results.params[0] > 0 else 1 / 10 ** 4\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    for col_num in range(0, X_trains[i].shape[1]):\n",
    "        X_in_for = pd.concat([X_in_for, X_trains[i].iloc[:, col_num]], axis = 1)\n",
    "\n",
    "        for p_value in [0.5, 0.2, 0.1]:\n",
    "            NB2_reg = sm.GLM(y_trains[i], X_in_for, family = sm.families.NegativeBinomial(alpha = block(y = y_trains[i], X = X_in_for)))\\\n",
    "            .fit()\n",
    "\n",
    "            signif_var = pd.DataFrame([x for x in NB2_reg.summary().tables[1].data[1:] if float(x[4]) < p_value],\n",
    "                                  columns = NB2_reg.summary().tables[1].data[0])\n",
    "\n",
    "            if len([*signif_var.iloc[:, 0]]) > 0:\n",
    "                X_in_for = X_in_for.loc[:, [*signif_var.iloc[:, 0]]]\n",
    "\n",
    "    k = 1\n",
    "    while k == 1:\n",
    "        p_values_of_NB2 = [float(x) for x in [*signif_var.iloc[:, 4]]]\n",
    "        max_p_value = max(p_values_of_NB2)\n",
    "\n",
    "        if max_p_value > 0.00001:\n",
    "            signif_vars_for_X = [*signif_var.iloc[:, 0]]\n",
    "            signif_vars_for_X.pop(p_values_of_NB2.index(max_p_value))\n",
    "\n",
    "            X_in_for = X_in_for.loc[:, signif_vars_for_X]\n",
    "\n",
    "            NB2_reg = sm.GLM(y_trains[i], X_in_for, family = sm.families.NegativeBinomial(alpha = block(y = y_trains[i], X = X_in_for)))\\\n",
    "                .fit()\n",
    "\n",
    "            signif_var = pd.DataFrame([x for x in NB2_reg.summary().tables[1].data[1:]],\n",
    "                                  columns = NB2_reg.summary().tables[1].data[0])\n",
    "        else: k = 0\n",
    "\n",
    "    return [*signif_var.iloc[:, 0]]\n",
    "\n",
    "NB2_signif_vars = Parallel(n_jobs = multiprocessing.cpu_count() - 3)\\\n",
    "    (delayed(NB2_fitting_i)(X_trains = X_trains, y_trains = y_trains, i = i) for i in tqdm(range(0, len(X_trains))))\n",
    "\n",
    "for j in [14, 15]:\n",
    "    NB2_signif_vars[j].remove('is_supplied_461504')\n",
    "    NB2_signif_vars[j].append('tar_is_supp')\n",
    "\n",
    "ZINBmodels = []\n",
    "for i in range(0, len(Xs)):\n",
    "    ZINBmodels.append(\n",
    "        sm.ZeroInflatedNegativeBinomialP(endog = y_trains[i],\n",
    "                               exog = X_trains[i][NB2_signif_vars[i]],\n",
    "                               exog_infl = X_trains[i][NB2_signif_vars[i]],\n",
    "                               inflation = 'logit').fit(maxiter = 100))\n",
    "\n",
    "ZINB_train_preds = [ZINBmodels[i].predict(X_trains[i][NB2_signif_vars[i]],\n",
    "                                          exog_infl = X_trains[i][NB2_signif_vars[i]])  for i  in range(0, len(Xs))]\n",
    "ZINB_test_preds = [ZINBmodels[i].predict(X_tests[i][NB2_signif_vars[i]],\n",
    "                                         exog_infl = X_tests[i][NB2_signif_vars[i]])  for i  in range(0, len(Xs))]\n",
    "\n",
    "MSE_tests = [metrics.mean_squared_error(y_tests[i], ZINB_test_preds[i]) for i in range(0, len(Xs))]\n",
    "print(f'MSE_tests_sum: {sum(MSE_tests)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}